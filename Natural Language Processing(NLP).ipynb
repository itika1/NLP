{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings, or word vectors, provide a way of mapping words from a vocabulary into a low-dimensional space, where words with similar meanings are close together. Let's play around with a set of pre-trained word vectors, to get used to their properties. There exist many sets of pretrained word embeddings; here, we use ConceptNet Numberbatch, which provides a relatively small download in an easy-to-work-with format (h5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "if not os.path.isfile('datasets/mini.h5'):\n",
    "    print(\"Downloading conceptnet numberbatch and word embeddings.....\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'datasets/mini.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use the package to open the mini.h5 file we just downloaded. We extract from the file a list of utf-8-encoded words, as well as their 300-d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimension : 362891\n",
      "all_embeddings dimensions : (362891, 300)\n",
      "Random example words : /c/fr/abstrait\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "with h5py.File('datasets/mini.h5', 'r') as F:\n",
    "    all_words = [word.decode('utf-8') for word in F['mat']['axis1'][:]]\n",
    "    all_embeddings = F['mat']['block0_values'][:]\n",
    "print(\"all_words dimension : {}\".format(len(all_words)))\n",
    "print(\"all_embeddings dimensions : {}\".format(all_embeddings.shape))\n",
    "print(\"Random example words : {}\".format(all_words[184576]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested only in the English words. We use Python list comprehensions to pull out the indices of the English words, then extract just the English words (stripping the six-character /c/en/ prefix) and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english words in all_words : 150875\n",
      "Embeddings of english words in all words : (150875, 300)\n",
      "Random example english words : coaching\n"
     ]
    }
   ],
   "source": [
    "english_words= [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_words_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embeddings = all_embeddings[english_words_indices]\n",
    "print(\"Number of english words in all_words : {0}\".format(len(english_words)))\n",
    "print(\"Embeddings of english words in all words : {0}\".format(english_embeddings.shape))\n",
    "print(\"Random example english words : {}\".format(english_words[25884]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of a word vector is less important than its direction; the magnitude can be thought of as representing frequency of use, independent of the semantics of the word. Here, we will be interested in semantics, so we normalize our vectors, dividing each by its length. The result is that all of our word vectors are length 1, and as such, lie on a unit circle. The dot product of two vectors is proportional to the cosine of the angle between them, and provides a measure of similarity (the bigger the cosine, the smaller the angle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "norms = np.linalg.norm(english_embeddings, axis=1)\n",
    "normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look up words easily, so we create a dictionary that maps us from a word to its index in the word embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= {word : i for i, word in enumerate(english_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to measure the similarity between pairs of words. We use numpy to take dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\trat\t 0.3766275\n",
      "cat\tmouse\t 0.31482112\n",
      "cat\tman\t -0.004878208\n",
      "cat\tmoo\t 0.0039538294\n",
      "cat\tfreeze\t -0.030225191\n",
      "antonym\topposite\t 0.39410648\n",
      "antonym\tantonym\t 0.9999999\n"
     ]
    }
   ],
   "source": [
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score\n",
    "\n",
    "# A word is as similar with itself as possible:\n",
    "print('cat\\trat\\t', similarity_score('cat', 'rat'))\n",
    "\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tmouse\\t', similarity_score('cat', 'mouse'))\n",
    "print('cat\\tman\\t', similarity_score('cat', 'man'))\n",
    "\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
    "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))\n",
    "\n",
    "# Antonyms are still considered related, sometimes more so than synonyms\n",
    "print('antonym\\topposite\\t', similarity_score('antonym', 'opposite'))\n",
    "print('antonym\\tantonym\\t', similarity_score('antonym', 'antonym'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to_vector(v, n):\n",
    "    all_scores = np.dot(normalized_embeddings, v)\n",
    "    best_words= list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))\n",
    "    return best_words[:n]\n",
    "def most_similar(w, n):\n",
    "    return closest_to_vector(normalized_embeddings[index[w], :], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'reticulated_python', 'pythons', 'pythonesque', 'scripting_language', 'ecmascript', 'php', 'objective_c', 'boa_constrictor', 'egyptian_cobra']\n",
      "['friend', 'schoolfriend', 'buddy', 'pal', 'colleague', 'friends', 'friended', 'best_friend', 'girlfriend', 'boyfriend']\n",
      "['spiderman', 'superhero', 'superman', 'ghost_rider', 'supermans', 'spider_man', 'captain_america', 'batman', 'superheroes', 'superheroine']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar('python', 10))\n",
    "print(most_similar('friend', 10))\n",
    "print(most_similar('spiderman', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use closest_to_vector to find words \"nearby\" vectors that we create ourselves. This allows us to solve analogies. For example, in order to solve the analogy \"man : brother :: woman : ?\", we can compute a new vector brother - man + woman: the meaning of brother, minus the meaning of man, plus the meaning of woman. We can then ask which words are closest, in the embedding space, to that new vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sister']\n",
      "['wife']\n",
      "['barcelona']\n"
     ]
    }
   ],
   "source": [
    "def solve_analogy(a1, b1, a2):\n",
    "    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]\n",
    "    return closest_to_vector(b2, 1)\n",
    "\n",
    "print(solve_analogy(\"man\", \"brother\", \"woman\"))\n",
    "print(solve_analogy(\"man\", \"husband\", \"woman\"))\n",
    "print(solve_analogy(\"juventus\", \"barcelona\", \"madrid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word embeddings in deep models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are fun to play around with, but their primary use is that they allow us to think of words as existing in a continuous, Euclidean space; we can then use an existing arsenal of techniques for machine learning with continuous numerical data (like logistic regression or neural networks) to process text. Let's take a look at an especially simple version of this. We'll perform sentiment analysis on a set of movie reviews: in particular, we will attempt to classify a movie review as positive or negative based on its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    \n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    \n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    \n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return x, y\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "xs = []\n",
    "ys = []\n",
    "with open(\"C:/Users/Itika/Desktop/movie-simple.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    for l in f.readlines():\n",
    "        x, y = convert_line_to_example(l)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "# Concatenate all examples into a numpy array\n",
    "xs = np.vstack(xs)\n",
    "ys = np.vstack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs: (1411, 300)\n",
      "Shape of labels: (1411, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of inputs: {}\".format(xs.shape))\n",
    "print(\"Shape of labels: {}\".format(ys.shape))\n",
    "\n",
    "num_examples = xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 labels before shuffling: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "First 20 labels after shuffling: [0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 20 labels before shuffling: {0}\".format(ys[:20, 0]))\n",
    "\n",
    "shuffle_idx = np.random.permutation(num_examples)\n",
    "xs = xs[shuffle_idx, :]\n",
    "ys = ys[shuffle_idx, :]\n",
    "\n",
    "print(\"First 20 labels after shuffling: {0}\".format(ys[:20, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "num_train = 4*num_examples //5\n",
    "x_train = torch.tensor(xs[:num_train])\n",
    "y_train = torch.tensor(ys[:num_train], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor(xs[num_train:])\n",
    "y_test = torch.tensor(ys[num_train:], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could format each batch individually as we feed it into the model, but to make it easier on ourselves, let's create a TensorDataset and DataLoader as we've used in the past for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "reviews_test = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build the model, organized as a nn.Module. We could make the number of outputs for our MLP the number of classes for this dataset (i.e. 2). However, since we only have two output classes here (\"positive\" vs \"negative\"), we can instead produce a single output value, calling everything greater than  0  \"postive\" and everything less than  0  \"negative\". If we pass this output through a sigmoid operation, then values are mapped to  [0,1] , with  0.5  being the classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we instantiate the model. Notice that since we are only doing binary classification, we use the binary cross-entropy (BCE) loss instead of the cross-entropy loss we've seen before. We use the \"with logits\" version for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train Loss: 0.6818224787712097 \t Train Acc: 0.5620567202568054\n",
      "Epoch: 25 \t Train Loss: 0.18264468014240265 \t Train Acc: 0.9539006948471069\n",
      "Epoch: 50 \t Train Loss: 0.07849441468715668 \t Train Acc: 0.972517728805542\n",
      "Epoch: 75 \t Train Loss: 0.08244629204273224 \t Train Acc: 0.9760638475418091\n",
      "Epoch: 100 \t Train Loss: 0.0745677500963211 \t Train Acc: 0.9822695255279541\n",
      "Epoch: 125 \t Train Loss: 0.02149335853755474 \t Train Acc: 0.9831560254096985\n",
      "Epoch: 150 \t Train Loss: 0.08886618167161942 \t Train Acc: 0.9858155846595764\n",
      "Epoch: 175 \t Train Loss: 0.05673982948064804 \t Train Acc: 0.9893617033958435\n",
      "Epoch: 200 \t Train Loss: 0.018002599477767944 \t Train Acc: 0.9955673813819885\n",
      "Epoch: 225 \t Train Loss: 0.015156297944486141 \t Train Acc: 0.9982269406318665\n",
      "Test accuracy: 0.9575971961021423\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "# Instantiate model\n",
    "model = SWEM()\n",
    "\n",
    "# Binary cross-entropy (BCE) Loss and Adam Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "for epoch in range(250):\n",
    "    correct = 0\n",
    "    num_examples = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        loss = criterion(y, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(y))\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_examples += len(inputs)\n",
    "    \n",
    "    # Print training progress\n",
    "    if epoch % 25 == 0:\n",
    "        acc = correct/num_examples\n",
    "        print(\"Epoch: {0} \\t Train Loss: {1} \\t Train Acc: {2}\".format(epoch, loss, acc))\n",
    "\n",
    "## Testing\n",
    "correct = 0\n",
    "num_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs \n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(y))\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_test += len(inputs)\n",
    "    \n",
    "print('Test accuracy: {}'.format(correct/num_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine what our model has learned, seeing how it responds to word vectors for different words:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of word boring : tensor([[2.2327e-18]], grad_fn=<SigmoidBackward>)\n",
      "Sentiment of word encouraging : tensor([[4.3608e-08]], grad_fn=<SigmoidBackward>)\n",
      "Sentiment of word entertaining : tensor([[1.0000]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "words_to_test= [\"boring\", \"encouraging\", \"entertaining\"]\n",
    "for word in words_to_test:\n",
    "    x = torch.tensor(normalized_embeddings[index[word]].reshape(1,300))\n",
    "    print(\"Sentiment of word {0} : {1}\".format(word, torch.sigmoid(model(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we learn word embeddings? To do so, we need to make them a part of our model, rather than as part of loading the data. In PyTorch, the preferred way to do so is with the nn.Embedding. Like the other nn layers we've seen (e.g. nn.Linear), nn.Embedding must be instantiated first. There are two required arguments for instantiation are the number of embeddings (i.e. the vocabulary size  ùëâ ) and the dimension of word embeddings (300, in our previous example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(500, 10)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 500\n",
    "embed_dimension= 10\n",
    "embedding = nn.Embedding(vocab_size , embed_dimension)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this matrix is basically a 10 dimensional word embedding for each of the 500 words, stacked on top of each other. Looking up a word embedding in this embedding matrix is simply selecting a specific row of this matrix, corresponding to the word.\n",
    "\n",
    "When word embeddings are learned, nn.Embedding look-up is often one of the first operations in a model module. For example, if we were to learn the word embeddings for our previous SWEM model, the model might instead look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWEMwithEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dimension, hidden_dimension, num_output):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size , embed_dimension)\n",
    "        self.fc1 = nn.Linear(embed_dimension, hidden_dimension)\n",
    "        self.fc2 = nn.Linear(hidden_dimension, num_output)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dimension=0)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've abstracted the size of the various layers of the model as constructor arguments, so we need to specify those hyperparameters at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWEMwithEmbeddings(\n",
      "  (embedding): Embedding(4000, 500)\n",
      "  (fc1): Linear(in_features=500, out_features=48, bias=True)\n",
      "  (fc2): Linear(in_features=48, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SWEMwithEmbeddings(\n",
    "        vocab_size = 4000,\n",
    "        embed_dimension = 500,\n",
    "        hidden_dimension = 48,\n",
    "        num_output = 4\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks(RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between sequential models and the previous models we've seen is the presence of a \"time\" dimension: words in a sentence (or paragraph, document) have an ordering to them that convey meaning:\n",
    "\n",
    "basic_RNN\n",
    "In the example sequence above, the word \"Recurrent\" is the  ùë°=1  word, which we denote  ùë§1 ; similarly, \"neural\" is  ùë§2 , and so on. As the preceding sections have hopefully impressed upon you, it is often more advantageous to model words as embedding vectors  ùë•1,...,ùë•ùëá , rather than one-hot vectors (which tokens  ùë§1,...ùë§ùëá  correspond to), so our first step is often to do an embedding table look-up for each input word. Let's assume 300-dimensional word embeddings and, for simplicity, a minibatch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs shape: torch.Size([5, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "mb = 1\n",
    "x_dim = 300 \n",
    "sentence = [\"recurrent\", \"neural\", \"network\", \"are\", \"great\"]\n",
    "\n",
    "xs = []\n",
    "for word in sentence:\n",
    "    xs.append(torch.tensor(normalized_embeddings[index[word]]).view(1, x_dim))\n",
    "    \n",
    "xs = torch.stack(xs, dim=0)\n",
    "print(\"xs shape: {}\".format(xs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we implement an RNN in PyTorch? There are quite a few ways, but let's build the Elman RNN from scratch first, using the input sequence \"recurrent neural networks are great\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an RNN, we project both the input  ùë•ùë°  and the previous hidden state  ‚Ñéùë°‚àí1  to some hidden dimension, which we're going to choose to be 128. To perform these operations, we're going to define some variables we're going to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 300]) torch.Size([300]) torch.Size([300, 300]) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "h_dim= 300\n",
    "\n",
    "# For projecting the input\n",
    "Wx= torch.randn(x_dim, h_dim)/np.sqrt(x_dim)\n",
    "Wx.requires_grad_()\n",
    "bx = torch.randn(h_dim, requires_grad=True)\n",
    "\n",
    "# For projecting the previous state\n",
    "Wh = torch.randn(h_dim, x_dim)/np.sqrt(h_dim)\n",
    "Wh.requires_grad_()\n",
    "bh= torch.randn(h_dim, requires_grad=True)\n",
    "\n",
    "print(Wx.shape, bx.shape, Wh.shape, bh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we define a function for one time step of the RNN. This function take the current input  ùë•ùë°  and previous hidden state  ‚Ñéùë°‚àí1 , performs the linear transformations  ùë•ùëäùë•+ùëèùë•  and  ‚Ñéùëä‚Ñé+ùëè‚Ñé , and then a hyperbolic tangent nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_step(x, h):\n",
    "    h_next = torch.tanh((torch.matmul(x, Wx) + bx) + (torch.matmul(h, Wh) + bh))\n",
    "\n",
    "    return h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embedding for the first word\n",
    "x1= xs[0, :, :]\n",
    "#initialize hidden state to 0\n",
    "h0 = torch.zeros([mb, h_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take one time step of the RNN, we call the function we wrote, passing in  ùë•1  and  ‚Ñé0 . In this case,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state h1 dimensions: torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass of one RNN step for time step t=1\n",
    "h1 = RNN_step(x1, h0)\n",
    "\n",
    "print(\"Hidden state h1 dimensions: {0}\".format(h1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state h2 dimension : torch.Size([1, 300])\n"
     ]
    }
   ],
   "source": [
    "#Word embedding for 2nd word\n",
    "x2 = xs[1, :, :]\n",
    "h2 = RNN_step(x2, h1)\n",
    "print(\"Hidden state h2 dimension : {0}\".format(h2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN parameter shapes: [torch.Size([300, 300]), torch.Size([300, 300]), torch.Size([300]), torch.Size([300])]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn\n",
    "\n",
    "rnn = nn.RNN(x_dim, h_dim)\n",
    "print(\"RNN parameter shapes: {}\".format([p.shape for p in rnn.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to perform a forward pass with an RNN, we pass the entire input sequence to the forward() function, which returns the hidden states at every time step (hs) and the final hidden state (h_T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states shape : torch.Size([5, 1, 300])\n",
      "Final hidden states shape : torch.Size([1, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "hs, h_T = rnn(xs)\n",
    "print(\"Hidden states shape : {}\".format(hs.shape))\n",
    "print(\"Final hidden states shape : {}\".format(h_T.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the RNNs we've just explored can successfully model simple sequential data, they tend to struggle with longer sequences, with vanishing gradients an especially big problem. A number of RNN variants have been proposed over the years to mitigate this issue and have been shown empirically to be more effective. In particular, Long Short-Term Memory (LSTM) and the Gated Recurrent Unit (GRU) have seen wide use recently in deep learning. We're not going to go into detail here about what structural differences they have from vanilla RNNs; a fantastic summary can be found here. Note that \"RNN\" as a name is somewhat overloaded: it can refer to both the basic recurrent model we went over previously, or recurrent models in general (including LSTMs and GRUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM parameters: [torch.Size([1200, 300]), torch.Size([1200, 300]), torch.Size([1200]), torch.Size([1200])]\n",
      "GRU parameters: [torch.Size([900, 300]), torch.Size([900, 300]), torch.Size([900]), torch.Size([900])]\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(x_dim, h_dim)\n",
    "print(\"LSTM parameters: {}\".format([p.shape for p in lstm.parameters()]))\n",
    "\n",
    "gru = nn.GRU(x_dim, h_dim)\n",
    "print(\"GRU parameters: {}\".format([p.shape for p in gru.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like PyTorch has Torchvision for computer vision, PyTorch also has Torchtext for natural language processing. As with Torchvision, Torchtext has a number of popular NLP benchmark datasets, across a wide range of tasks (e.g. sentiment analysis, language modeling, machine translation). It also has a few pre-trained word embeddings available as well, including the popular Global Vectors for Word Representation (GloVe). If you need to load your own dataset, Torchtext has a number of useful containers that can make the data pipeline easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
